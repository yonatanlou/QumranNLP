{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2520625",
   "metadata": {},
   "source": [
    "## Unsupervised clustering using GAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc79f93",
   "metadata": {},
   "source": [
    "Graph neural networks (GNNs) have received a fair amount of attention over the past few years. That said, some of the initial excitement has faded-especially in certain research domains. Part of this decline is due to the rise of transformer models, which in many ways behave like fully connected GNNs. This has led some people to question whether GNNs are still relevant or necessary. ([Transformers vs GNNs â€“ Taewoon Kim](https://taewoon.kim/2024-10-15-transformer-vs-gnn/), [Transformers are GNNs â€“ Graph Deep Learning](https://graphdeeplearning.github.io/post/transformers-are-gnns/), [Reddit discussion: Are GNNs obsolete?](https://www.reddit.com/r/MachineLearning/comments/1jgwjjk/d_are_gnns_obsolete_because_of_transformers/)).\n",
    "\n",
    "\n",
    "Personally, I still find GNNs extremely useful-particularly in two situations:\n",
    "\n",
    "1. When your data naturally forms a graph.\n",
    "2. When you want to combine multiple types of features in a \"learnable\" and flexible way.\n",
    "\n",
    "In this post, Iâ€™ll walk through how to implement **unsupervised text clustering** using a **[Graph Autoencoder (GAE)](https://arxiv.org/abs/1611.07308)** framework that supports multiple feature types.\n",
    "\n",
    "This is more of a quick-and-dirty prototype than a polished package. I wrote it mainly because I couldnâ€™t find a simple example of unsupervised text clustering using GNNs online.\n",
    "\n",
    "If you're looking for a more customizable and production-ready version, you can check out the [`QumranNLP`](https://github.com/yonatanlou/QumranNLP) repository. It's built around a fascinating dataset-texts from the Dead Sea Scrolls-and uses a more refined version of the same approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc3c47",
   "metadata": {},
   "source": [
    "First of all, we will import some important libraries, make some constants (which can be optimize in the future), and collect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81392150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loading 20â€‘Newsgroups â€¦\n"
     ]
    }
   ],
   "source": [
    "import torch, random, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import v_measure_score, adjusted_rand_score\n",
    "from torch_geometric.nn import GAE, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "\n",
    "SAMPLES = 1000           # subset size\n",
    "N_CLUST = 20            # kâ€‘means clusters (20â€‘news groups)\n",
    "HIDDEN  = 256           # GCN hidden dim\n",
    "LATENT  = 128           # GCN latent dim\n",
    "LR      = 0.001         # learning rate\n",
    "EPOCHS  = 350           # training epochs\n",
    "DEVICE  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "Q_SIM     = 0.999\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "print(\"Step 1 | Loading 20â€‘Newsgroups â€¦\")\n",
    "news = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "texts, y = news.data[:SAMPLES], news.target[:SAMPLES]\n",
    "\n",
    "\n",
    "def set_seed_globally(seed=42):\n",
    "    # Set seeds\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed_globally(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ad540",
   "metadata": {},
   "source": [
    "Now, we'll represent the text using three different methods. These are just examples-you can easily swap them out or tweak the configurations to fit your own data or preferences.\n",
    "\n",
    "The methods we'll use are:\n",
    "\n",
    "- **BERT embeddings** â€“ contextual representations from a pretrained language model.\n",
    "- **TF-IDF** â€“ a classic, sparse representation that captures term importance across the corpus.\n",
    "- **Character n-grams** â€“ helpful for capturing subword patterns, especially in noisy texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5690f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | DistilBERT embeddings â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | TFâ€‘IDF & charâ€‘3â€‘gram â€¦\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2 | DistilBERT embeddings â€¦\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model_bert = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(DEVICE).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def bert_embed(docs, bs=16):\n",
    "    out = []\n",
    "    for i in tqdm(range(0, len(docs), bs), desc=\"Embedding\", leave=False):\n",
    "        batch = docs[i:i+bs]\n",
    "        inp = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "        out.append(model_bert(**inp).last_hidden_state.mean(dim=1).cpu())\n",
    "    return torch.cat(out).numpy()\n",
    "\n",
    "Xb = bert_embed(texts)\n",
    "\n",
    "\n",
    "print(\"Step 3 | TFâ€‘IDF & charâ€‘3â€‘gram â€¦\")\n",
    "Xt = TfidfVectorizer(max_features=1500).fit_transform(texts).toarray()\n",
    "Xn = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3), max_features=1500).fit_transform(texts).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf6b02",
   "metadata": {},
   "source": [
    "At this point, we're building the graph based on the TF-IDF and character n-gram features.\n",
    "\n",
    "There are quite a few parameters you can tweak here, and each choice can significantly affect your model. Some key considerations:\n",
    "\n",
    "- **Similarity metric**: How do we calculate the similarity between vectors? Common options are cosine similarity and Euclidean distance.\n",
    "- **Graph structure**: Do we want a **heterogeneous graph** (with multiple edge types, one for each feature type), or a **homogeneous graph** (a single adjacency matrix that combines all features)?\n",
    "\n",
    "These decisions give you a lot of flexibility-and room for creativity-to improve your model.\n",
    "\n",
    "> ðŸ”§ **Note:** One of the most critical parameters is the similarity threshold for edge creation (`Q_SIM`).  \n",
    "If this threshold is set too low, youâ€™ll end up with a massive graph-which means you'll need a lot of GPU's just to train the model.  \n",
    "Through trial and error, Iâ€™ve found that using a **higher threshold** often results in **better performance** and **faster training**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d93e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Building graph edges (kâ€‘NN) â€¦\n",
      "   TF-IDF edges:  1032\n",
      "   N-gram edges: 1028\n",
      "   Combined edges: 1056\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4 | Building graph edges (kâ€‘NN) â€¦\")\n",
    "\n",
    "def adj_cosine(mat, q=0.99):\n",
    "    norm = mat / (np.linalg.norm(mat, axis=1, keepdims=True) + 1e-8)\n",
    "    sim  = norm @ norm.T\n",
    "    thresh = np.quantile(sim, q)\n",
    "    adj = (sim >= thresh).astype(float)\n",
    "    np.fill_diagonal(adj, 1)\n",
    "    return adj\n",
    "\n",
    "def adj_to_edge(adj):\n",
    "    src, dst = np.nonzero(adj)\n",
    "    return to_undirected(torch.tensor([src, dst], dtype=torch.long))\n",
    "\n",
    "adj_tfidf = adj_cosine(Xt, Q_SIM)\n",
    "adj_ngram = adj_cosine(Xn, Q_SIM)\n",
    "adj_comb  = ((adj_tfidf + adj_ngram) > Q_SIM).astype(float)  # union\n",
    "print(f\"   TF-IDF edges:  {int(adj_tfidf.sum())}\")\n",
    "print(f\"   N-gram edges: {int(adj_ngram.sum())}\")\n",
    "print(f\"   Combined edges: {int(adj_comb.sum())}\")\n",
    "E = adj_to_edge(adj_comb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341208",
   "metadata": {},
   "source": [
    "Now we move on to training the model.  \n",
    "In my original implementation, I included early stopping to avoid overfitting-but for the sake of this simplified version, I skipped it (I was lazy ðŸ˜…).\n",
    "\n",
    "Just like before, this part is highly customizable. You can experiment with:\n",
    "\n",
    "- The number of layers\n",
    "- Hidden dimensions\n",
    "- Dropout rates\n",
    "- Batch normalization\n",
    "- Activation functions\n",
    "\n",
    "Feel free to design the GAE/VGAE architecture in a way that fits your data and goals.\n",
    "\n",
    "---\n",
    "\n",
    "Evaluating unsupervised clustering models is always a bit of a mystery. There's no single \"correct\" metric, and depending on your application, some may be more meaningful than others.  \n",
    "Still, I think [Scikit-learnâ€™s guide on Clustering Performance Evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) is one of the best overviews available online.\n",
    "\n",
    "I also wrote about a more niche but useful method in my post on  \n",
    "[Evaluating Hierarchical Clustering](https://yonatanlou.github.io/blog/Evaluating-Hierarchical-Clustering/hierarchical-clustering-eval/), which dives into metrics like the Dasgupta cost (specific for hierarchial clustering).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cbc55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Graph Autoâ€‘Encoder â€¦\n",
      "Epoch 010 | Loss 1.1351\n",
      "Epoch 020 | Loss 1.0478\n",
      "Epoch 030 | Loss 0.9715\n",
      "Epoch 040 | Loss 0.8702\n",
      "Epoch 050 | Loss 0.8832\n",
      "Epoch 060 | Loss 0.8712\n",
      "Epoch 070 | Loss 0.8518\n",
      "Epoch 080 | Loss 0.8291\n",
      "Epoch 090 | Loss 0.8149\n",
      "Epoch 100 | Loss 0.7946\n",
      "Epoch 110 | Loss 0.8166\n",
      "Epoch 120 | Loss 0.8010\n",
      "Epoch 130 | Loss 0.7978\n",
      "Epoch 140 | Loss 0.7979\n",
      "Epoch 150 | Loss 0.8014\n",
      "Epoch 160 | Loss 0.8089\n",
      "Epoch 170 | Loss 0.7826\n",
      "Epoch 180 | Loss 0.7878\n",
      "Epoch 190 | Loss 0.8120\n",
      "Epoch 200 | Loss 0.7809\n",
      "Epoch 210 | Loss 0.7806\n",
      "Epoch 220 | Loss 0.7765\n",
      "Epoch 230 | Loss 0.7945\n",
      "Epoch 240 | Loss 0.7801\n",
      "Epoch 250 | Loss 0.7783\n",
      "Epoch 260 | Loss 0.7951\n",
      "Epoch 270 | Loss 0.7917\n",
      "Epoch 280 | Loss 0.7733\n",
      "Epoch 290 | Loss 0.7740\n",
      "Epoch 300 | Loss 0.7602\n",
      "Epoch 310 | Loss 0.7769\n",
      "Epoch 320 | Loss 0.7720\n",
      "Epoch 330 | Loss 0.7843\n",
      "Epoch 340 | Loss 0.7836\n",
      "Epoch 350 | Loss 0.7654\n",
      "Step 6 | Clustering latent space â€¦\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Step 5 | Training Graph Autoâ€‘Encoder â€¦\")\n",
    "\n",
    "graph = Data(x=torch.tensor(Xb, dtype=torch.float), edge_index=E)\n",
    "graph = graph.to(DEVICE)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_z):\n",
    "        super().__init__()\n",
    "        self.g1 = GCNConv(dim_in, dim_h)\n",
    "        self.g2 = GCNConv(dim_h, dim_z)\n",
    "    def forward(self, x, ei):\n",
    "        return self.g2(self.g1(x, ei).relu(), ei)\n",
    "\n",
    "gae = GAE(Encoder(graph.x.size(1), HIDDEN, LATENT)).to(DEVICE)\n",
    "opt = torch.optim.Adam(gae.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    gae.train(); opt.zero_grad()\n",
    "    z = gae.encode(graph.x, graph.edge_index)\n",
    "    loss = gae.recon_loss(z, graph.edge_index)\n",
    "    loss.backward(); opt.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce1acd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 | Clustering latent space â€¦\n",
      "\n",
      "Results\n",
      "Baseline (BERT + kâ€‘means) â†’ V: 0.317 | ARI: 0.124\n",
      "GAE (BERT + TF-IDF + N-grams)   â†’ V: 0.355 | ARI: 0.153\n",
      "Improvement: V 12.082% | ARI 23.501%\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6 | Clustering latent space â€¦\")\n",
    "gae.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = gae.encode(graph.x, graph.edge_index).cpu().detach().numpy()\n",
    "    km_emb  = KMeans(N_CLUST, n_init=10).fit(embeddings)\n",
    "    gae_v   = v_measure_score(y, km_emb.labels_)\n",
    "    gae_ari = adjusted_rand_score(y, km_emb.labels_)\n",
    "\n",
    "km_base = KMeans(N_CLUST, n_init=10).fit(Xb)\n",
    "base_v   = v_measure_score(y, km_base.labels_)\n",
    "base_ari = adjusted_rand_score(y, km_base.labels_)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nResults\")\n",
    "print(f\"Baseline (BERT + kâ€‘means) â†’ V: {base_v:.3f} | ARI: {base_ari:.3f}\")\n",
    "print(f\"GAE (BERT + TF-IDF + N-grams)   â†’ V: {gae_v:.3f} | ARI: {gae_ari:.3f}\")\n",
    "print(f\"Improvement: V {(gae_v/base_v)-1:.3%} | ARI {(gae_ari/base_ari)-1:.3%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QumranNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
